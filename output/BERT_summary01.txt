Managing Machines: the governance of artificial intelligence Speech given by James Proudman, Executive Director of UK Deposit Takers Supervision FCA Conference on Governance in Banking I am very grateful to Philip Sellar for preparing these remarks, and to Sadia Arif, Jamie Barber, Jas Ellis, Magnus Falk, Orlando Fernandez Ruiz, Anna Jernova, Carsten Jung, Tom Mutton, Lyndon Nelson, Jennifer Nemeth and Oliver Thew for very helpful advice and comments. Within a year, the company realised its new system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way. Most came from men, a reflection of male dominance across the technology industry. The story is a clear example of how artificial intelligence can produce bad outcomes for all concerned. It also offers a case study for exploring the root causes that lead to bad outcomes - and so in turn offers insights for boards on how to govern the introduction of artificial intelligence. Artificial Intelligence Technological innovation is inevitable and welcome. As the Governor noted during this year’s UK Fintech Week, we are shifting towards a new economy, which is powered by big data, advanced analytics, smartphone technology and more distributed peer-to-peer connections. It is certainly not the role of the regulator to stand in the way of progress. In 2018, an FT survey of banks around the world provided evidence of a cautious approach being taken by firms. Third, the extent of barriers - regulatory or otherwise - to adoption and what techniques and tools can enable safe use of this technology. Consistent with the McKinsey survey, barriers to AI deployment currently seem to be mostly internal to firms, rather than stemming from regulation. Some of the main reasons include: (i) legacy systems and unsuitable IT infrastructure; (ii) lack of access to sufficient data; and (iii) challenges integrating ML into existing business processes. But many say that new approaches to model validation (which include AI explainability techniques) are needed in the future. Of the firms regulated by the Bank of England that responded to the survey, 57 per cent reported that they are using AI applications in risk management and compliance areas, including anti-fraud and anti-money laundering applications. Challenges of AI and ML for boards Let me suggest that there are three challenges for boards and management. Further, there are complex ethical, legal, conduct and reputational issues associated with the use of personal data. From a regulatory perspective, they are perhaps more directly a primary concern to the FCA given its statutory objectives of consumer protection, but are also potentially relevant to safety and soundness, not least through their impact on reputation and, in turn, confidence. In a more automated, fast-moving world of AI/ML, boards – not just regulators – will need to consider and be on top of these issues. The third challenge posed by greater use of AI/ML to boards is around change. It appears to supervisors, and consistent with the early results from the Bank of England/FCA survey, that some firms are approaching the introduction of AI/ML piecemeal, project by project; others appear to be following a more integrated, strategic approach. Transition may also create complex interdependencies between the parts of firms that are often thought of, and treated as, largely separate. As the use of technology changes, the impact on staff roles, skills and evaluation may be equally profound. Artificial Intelligence and Data Analytics in Singapore’s Financial Sector’ And third, the acceleration in the rate of introduction of AI/ML will create increased execution risks during the transition that need to be overseen. Boards should reflect on the range of skill sets and controls that are required to mitigate these risks both at senior level and throughout the organisation.